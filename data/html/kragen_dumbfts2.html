<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>kragen_dumbfts2</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<ol start="3" type="1">
<li>The corpus size is in the tens-of-megabytes to tens-of-gigabytes range.</li>
<li>The mbox file is updated only by appending onto the end. (This is a reasonable assumption given #1 and #3.)</li>
<li>Given #3, incremental reindexing is desirable. Given #4, it is feasible.</li>
<li>You have enough disk space for an index that is the same size as the original mbox file.</li>
<li>The mail and the index are stored on a spinning-rust electromechanical disk, on which non-sequential reads imply a delay of 5–20 milliseconds, not a solid-state Flash disk (“SSD”) where the corresponding delay is around 0.1 milliseconds. (It should still work fine on a Flash disk but it could be a lot more efficient.)</li>
<li>You don’t care about full-text search finding words in attachments or base64-encoded message bodies, or partially-encoded words in quoted-printable-encoded message bodies, or encoded message headers.</li>
<li>You’re on a Unix system, such as Linux or Mac OS X.</li>
<li>You don’t mind manually administering the index.</li>
<li>The index is being used to answer interactive queries on a single-user machine with a single-user corpus, so the thing to optimize for is query latency when the index is on disk, not, say, query throughput or query latency with an in-RAM index.</li>
</ol>
<h2 id="architecture">Architecture</h2>
<p>It stores a bunch of index segments in an index directory; each one is a sorted list of postings. Off to the side of each index segment, there is a “skip file” which is smaller, about 1400 times smaller in my case, although that’s a tunable parameter; it contains about one out of every 1400 or so postings from the index segment, along with the position where it can be found in the index segment. Index segments are generated at some manageable size and then merged into new, larger index segments later on.</p>
<p>If this design sounds familiar it’s probably because it’s exactly like Lucene.</p>
<p>So, to find a term in a 2.0-gibibyte index segment, we start reading through the skip file (1.3 mebibytes in my example case), and stop as soon as we find something later than the term, on average a little over halfway through. At that point we open the index segment, seek to the last posting mentioned in the skip file before the term we’re seeking, and then read sequentially through the index file until we find all the postings we’re looking for.</p>
<p>Most terms have relatively few postings — in fact, the vast majority have only a single posting, although those tend not to get searched for — so we will usually have to read less than 1400 postings from the</p>
</body>
</html>
