<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>bwaldvogel_liblinear-java4</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<pre><code>        |f&#39;(w)|_1 &lt;= eps*min(pos,neg)/l*|f&#39;(w0)|_1,
        where f is the primal function (default 0.01)
    -s 12 and 13\n&quot;
        |f&#39;(alpha)|_1 &lt;= eps |f&#39;(alpha0)|,
        where f is the dual function (default 0.1)
-B bias : if bias &gt;= 0, instance x becomes [x; bias]; if &lt; 0, no bias term added (default -1)
-wi weight: weights adjust the parameter C of different classes (see README for details)
-v n: n-fold cross validation mode
-C : find parameter C (only for -s 0 and 2)
-q : quiet mode (no outputs)</code></pre>
<p>Option -v randomly splits the data into n parts and calculates cross validation accuracy on them.</p>
<p>Option -C conducts cross validation under different C values and finds the best one. This options is supported only by -s 0 and -s 2. If the solver is not specified, -s 2 is used.</p>
<p>Formulations:</p>
<p>For L2-regularized logistic regression (-s 0), we solve</p>
<pre><code>min_w w^Tw/2 + C \sum log(1 + exp(-y_i w^Tx_i))</code></pre>
<p>For L2-regularized L2-loss SVC dual (-s 1), we solve</p>
<pre><code>min_alpha  0.5(alpha^T (Q + I/2/C) alpha) - e^T alpha
    s.t.   0 &lt;= alpha_i,</code></pre>
<p>For L2-regularized L2-loss SVC (-s 2), we solve</p>
<pre><code>min_w w^Tw/2 + C \sum max(0, 1- y_i w^Tx_i)^2</code></pre>
<p>For L2-regularized L1-loss SVC dual (-s 3), we solve</p>
<pre><code>min_alpha  0.5(alpha^T Q alpha) - e^T alpha
    s.t.   0 &lt;= alpha_i &lt;= C,</code></pre>
<p>For L1-regularized L2-loss SVC (-s 5), we solve</p>
<pre><code>min_w \sum |w_j| + C \sum max(0, 1- y_i w^Tx_i)^2</code></pre>
<p>For L1-regularized logistic regression (-s 6), we solve</p>
<pre><code>min_w \sum |w_j| + C \sum log(1 + exp(-y_i w^Tx_i))</code></pre>
<p>For L2-regularized logistic regression (-s 7), we solve</p>
<pre><code>min_alpha  0.5(alpha^T Q alpha) + \sum alpha_i*log(alpha_i) + \sum (C-alpha_i)*log(C-alpha_i) - a constant
    s.t.   0 &lt;= alpha_i &lt;= C,</code></pre>
</body>
</html>
