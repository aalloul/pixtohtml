<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>bwaldvogel_liblinear-java6</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<p>===============</p>
<pre><code>Usage: predict [options] test_file model_file output_file
options:
-b probability_estimates: whether to output probability estimates, 0 or 1 (default 0); currently for logistic regression only
-q : quiet mode (no outputs)</code></pre>
<p>Note that -b is only needed in the prediction phase. This is different from the setting of LIBSVM.</p>
<h1 id="examples">Examples</h1>
<pre><code>&gt; train data_file</code></pre>
<p>Train linear SVM with L2-loss function.</p>
<pre><code>&gt; train -s 0 data_file</code></pre>
<p>Train a logistic regression model.</p>
<pre><code>&gt; train -v 5 -e 0.001 data_file</code></pre>
<p>Do five-fold cross-validation using L2-loss SVM. Use a smaller stopping tolerance 0.001 than the default 0.1 if you want more accurate solutions.</p>
<pre><code>&gt; train -C data_file</code></pre>
<p>Conduct cross validation many times by L2-loss SVM and find the parameter C which achieves the best cross validation accuracy.</p>
<pre><code>&gt; train -C -s 0 -v 3 -c 0.5 -e 0.0001 data_file</code></pre>
<p>For parameter selection by -C, users can specify other solvers (currently -s 0 and -s 2 are supported) and different number of CV folds. Further, users can use the -c option to specify the smallest C value of the search range. This setting is useful when users want to rerun the parameter selection procedure from a specified C under a different setting, such as a stricter stopping tolerance -e 0.0001 in the above example.</p>
<pre><code>&gt; train -c 10 -w1 2 -w2 5 -w3 2 four_class_data_file</code></pre>
<p>Train four classifiers: positive negative Cp Cn class 1 class 2,3,4. 20 10 class 2 class 1,3,4. 50 10</p>
</body>
</html>
