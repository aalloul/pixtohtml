<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>samuelclay_NewsBlur7</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<pre><code>Typically it&#39;ll be mongo, but any of the redis or postgres servers can be unreachable due to
acts of god. Otherwise, a frequent cause is lack of disk space. There are monitors on every DB
server watching for disk space, emailing me when they&#39;re running low, but it still happens.</code></pre>
<ol start="4" type="1">
<li><p>Check the various databases:</p>
<ol type="a">
<li><p>If Redis server (db_redis, db_redis_story, db_redis_pubsub) can’t connect, redis is probably down.</p>
<p>SSH into the offending server (or just check both the <code>db_redis</code> and <code>db_redis_story</code> servers) and check if <code>redis</code> is running. You can often <code>tail -f -n 100 /var/log/redis.log</code> to find out if background saving was being SIG(TERM|INT)’ed. When redis goes down, it’s always because it’s consuming too much memory. That shouldn’t happen, so check the <a href="http://db_redis/munin/">munin graphs</a>.</p>
<p>Boot it with <code>sudo /etc/init.d/redis start</code>.</p></li>
<li><p>If mongo (db_mongo) can’t connect, mongo is probably down.</p>
<p>This is rare and usually signifies hardware failure. SSH into <code>db_mongo</code> and check logs with <code>tail -f -n 100 /var/log/mongodb/mongodb.log</code>. Start mongo with <code>sudo /etc/init.d/mongodb start</code> then promote the next largest mongodb server. You want to then promote one of the secondaries to primary, kill the offending primary machine, and rebuild it (preferably at a higher size). I recommend waiting a day to rebuild it so that you get a different machine. Don’t forget to lodge a support ticket with the hosting provider so they know to check the machine.</p>
<p>If it’s the db_mongo_analytics machine, there is no backup nor secondaries of the data (because it’s ephemeral and used for, you guessed it, analytics). You can easily provision a new mongodb server and point to that machine.</p>
<p>If mongo is out of space, which happens, the servers need to be re-synced every 2-3 months to compress the data bloat. Simply <code>rm -fr /var/lib/mongodb/*</code> and re-start Mongo. It will re-sync.</p>
<p>If both secondaries are down, then the primary Mongo will go down. You’ll need a secondary mongo in the sync state at the very least before the primary will accept reads. It shouldn’t take long to get into that state, but you’ll need a mongodb machine setup. You can immediately reuse the non-working secondary if disk space is the only issue.</p></li>
<li><p>If postgresql (db_pgsql) can’t connect, postgres is probably down.</p>
<p>This is the rarest of the rare and has in fact never happened. Machine failure. If you can salvage the db data, move it to another machine. Worst case you have nightly backups in S3. The fabfile.py has commands to assist in restoring from backup (the backup file just needs to be local).</p></li>
</ol></li>
<li><p>Point to a new/different machine</p>
<ol type="a">
<li><p>Confirm the IP address of the new machine with <code>fab list_do</code>.</p></li>
<li><p>Change <code>secrets-newsbur/config/hosts</code> to reflect the new machine.</p></li>
<li><p>Copy the new <code>hosts</code> file to all machines with:</p></li>
</ol></li>
</ol>
</body>
</html>
